{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "# Boston Housing - Price Prediction with Amazon SageMaker\n",
    "\n",
    "In this notebook we will predict house prices based on the well-known Boston Housing dataset with a simple regression model in Tensorflow 2. This public dataset contains 13 features regarding housing stock of towns in the Boston area. Features include average number of rooms, accessibility to radial highways, adjacency to a major river, etc.\n",
    "\n",
    "This notebook includes all key steps such as preprocessing data with SageMaker Processing, and model training and deployment with SageMaker hosted training and inference. Automatic Model Tuning in SageMaker is used to tune the model's hyperparameters. If you are using TensorFlow 2, you can use the Amazon SageMaker prebuilt TensorFlow 2 framework container with training scripts similar to those you would use outside SageMaker.\n",
    "\n",
    "To begin, we'll import some necessary packages and set up directories for training and test data. We'll also set up a SageMaker Session to perform various operations, and specify an Amazon S3 bucket to hold input data and output. The default bucket used here is created by SageMaker if it doesn't already exist, and named in accordance with the AWS account ID and AWS Region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Data preprocessing\n",
    "\n",
    "We'll import the dataset and transform it with SageMaker Processing, which can be used to process terabytes of data in a SageMaker-managed cluster separate from the instance running your notebook server. In a typical SageMaker workflow, notebooks are only used for prototyping and can be run on relatively inexpensive and less powerful instances, while processing, training and model hosting tasks are run on separate, more powerful SageMaker-managed instances.  SageMaker Processing includes off-the-shelf support for Scikit-learn, as well as a Bring Your Own Container option, so it can be used with many different data transformation technologies and tasks.  An alternative to SageMaker Processing is [SageMaker Data Wrangler](https://aws.amazon.com/sagemaker/data-wrangler/), a visual data preparation tool integrated with the SageMaker Studio UI.    \n",
    "\n",
    "To work with SageMaker Processing, first we'll load the Boston Housing dataset, save the raw feature data and upload it to Amazon S3 so it can be accessed by SageMaker Processing.  We'll also save the labels for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "session = sagemaker.session.Session()\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "np.save(os.path.join(data_dir, 'X.npy'), X)\n",
    "np.save(os.path.join(data_dir, 'y.npy'), y)\n",
    "\n",
    "dataset_location = session.upload_data(path=data_dir, key_prefix='boston-housing/data/raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-eu-west-1-663383416343/boston-housing/data/raw\n"
     ]
    }
   ],
   "source": [
    "print(dataset_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Next, simply supply an ordinary Python data preprocessing script as shown below.  For this example, we're using a SageMaker prebuilt Scikit-learn framework container, which includes many common functions for processing data.  There are few limitations on what kinds of code and operations you can run, and only a minimal API contract:  input and output data must be placed in specified directories.  If this is done, SageMaker Processing automatically loads the input data from S3 and uploads transformed data back to S3 when the job is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Before starting the SageMaker Processing job, we instantiate a `SKLearnProcessor` object.  This object allows you to specify the instance type to use in the job, as well as how many instances.  Spinning a cluster is just a matter of setting `instahce_count` to 2 or more, but our transformation has a `StandardScaler` which must be run over all training data and applied equally to train and test data. That can't be parallelized with `scikit-learn`, but since the dataset is small, that is not a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "execution_role = get_execution_role()\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(framework_version='0.23-1',\n",
    "                                     role=execution_role,\n",
    "                                     instance_type='ml.m5.xlarge',\n",
    "                                     instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "We're now ready to run the Processing job.  To enable distributing the data files equally among the instances, we specify the `ShardedByS3Key` distribution type in the `ProcessingInput` object.  This ensures that if we have `n` instances, each instance will receive `1/n` files from the specified S3 bucket.  It may take around 3 minutes for the following code cell to run, mainly to set up the cluster.  At the end of the job, the cluster automatically will be torn down by SageMaker.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-scikit-learn-2021-04-29-17-58-10-145\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-eu-west-1-663383416343/boston-housing/data/raw', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-eu-west-1-663383416343/sagemaker-scikit-learn-2021-04-29-17-58-10-145/input/code/preprocessing.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'output-1', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-eu-west-1-663383416343/boston-housing/data/processed', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n",
      "........................\n",
      "\u001b[34mINPUT FILE LIST: \u001b[0m\n",
      "\u001b[34m['/opt/ml/processing/input/y.npy', '/opt/ml/processing/input/X.npy']\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mFit & transform features...\u001b[0m\n",
      "\u001b[34mDone!\u001b[0m\n",
      "\u001b[34mSaved X_train file...\n",
      "\u001b[0m\n",
      "\u001b[34mSaved y_train file...\n",
      "\u001b[0m\n",
      "\u001b[34mSaved X_test file...\n",
      "\u001b[0m\n",
      "\u001b[34mSaved y_test file...\n",
      "\u001b[0m\n",
      "\u001b[34mSaved X_val file...\n",
      "\u001b[0m\n",
      "\u001b[34mSaved y_val file...\n",
      "\u001b[0m\n",
      "\u001b[34mSaved StandardScaler file...\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "bucket = session.default_bucket() \n",
    "\n",
    "processed_dataset = 's3://{}/boston-housing/data/processed'.format(bucket)\n",
    "\n",
    "sklearn_processor.run(\n",
    "    code='code/preprocessing/preprocessing.py',\n",
    "    inputs=[ProcessingInput(\n",
    "        source=dataset_location,\n",
    "        destination='/opt/ml/processing/input'\n",
    "    )],\n",
    "    outputs=[ProcessingOutput(\n",
    "        source='/opt/ml/processing/output',\n",
    "        destination=processed_dataset\n",
    "    )]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "In the log output of the SageMaker Processing job above, you should be able to see logs in two different colors for the two different instances, and that each instance received different files.  Without the `ShardedByS3Key` distribution type, each instance would have received a copy of **all** files.  By spreading the data equally among `n` instances, you should receive a speedup by approximately a factor of `n` for most stateless data transformations.  After saving the job results locally, we'll move on to training and inference code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Training\n",
    "\n",
    "Now that we've prepared a dataset, we can move on to SageMaker's model training functionality. With SageMaker hosted training the actual training itself occurs not on the notebook instance, but on a separate cluster of machines managed by SageMaker. Before starting hosted training, the data must be in S3, or an EFS or FSx for Lustre file system. We'll upload to S3 now, and confirm the upload was successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "We're now ready to set up an Estimator object for hosted training. We simply call `fit` to start the actual hosted training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "hyperparameters = {'epochs': 50, 'batch_size': 128, 'learning_rate': 0.01}\n",
    "\n",
    "hosted_estimator = TensorFlow(\n",
    "                       source_dir='code/train',\n",
    "                       entry_point='train.py',\n",
    "                       instance_type='ml.c5.xlarge',\n",
    "                       instance_count=1,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=sagemaker.get_execution_role(),\n",
    "                       framework_version='2.3.1',\n",
    "                       py_version='py37')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "After starting the hosted training job with the `fit` method call below, you should observe the valication loss converge with each epoch.  Can we do better? We'll look into a way to do so in the **Automatic Model Tuning** section below. In the meantime, the hosted training job should take about 3 minutes to complete.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-29 18:02:24 Starting - Starting the training job...\n",
      "2021-04-29 18:02:46 Starting - Launching requested ML instancesProfilerReport-1619719344: InProgress\n",
      "......\n",
      "2021-04-29 18:03:52 Starting - Preparing the instances for training......\n",
      "2021-04-29 18:04:50 Downloading - Downloading input data\n",
      "2021-04-29 18:04:50 Training - Downloading the training image...\n",
      "2021-04-29 18:05:20 Uploading - Uploading generated training model\u001b[34m2021-04-29 18:05:10.018616: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2021-04-29 18:05:10.022164: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2021-04-29 18:05:10.159096: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2021-04-29 18:05:12,392 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2021-04-29 18:05:12,399 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-04-29 18:05:12,722 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-04-29 18:05:12,738 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-04-29 18:05:12,752 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-04-29 18:05:12,763 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"input\": \"/opt/ml/input/data/input\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 128,\n",
      "        \"model_dir\": \"s3://sagemaker-eu-west-1-663383416343/tensorflow-training-2021-04-29-18-02-23-924/model\",\n",
      "        \"epochs\": 50,\n",
      "        \"learning_rate\": 0.01\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"input\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"tensorflow-training-2021-04-29-18-02-23-924\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-663383416343/tensorflow-training-2021-04-29-18-02-23-924/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":128,\"epochs\":50,\"learning_rate\":0.01,\"model_dir\":\"s3://sagemaker-eu-west-1-663383416343/tensorflow-training-2021-04-29-18-02-23-924/model\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"input\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"input\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-west-1-663383416343/tensorflow-training-2021-04-29-18-02-23-924/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"input\":\"/opt/ml/input/data/input\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":128,\"epochs\":50,\"learning_rate\":0.01,\"model_dir\":\"s3://sagemaker-eu-west-1-663383416343/tensorflow-training-2021-04-29-18-02-23-924/model\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"input\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tensorflow-training-2021-04-29-18-02-23-924\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-663383416343/tensorflow-training-2021-04-29-18-02-23-924/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"128\",\"--epochs\",\"50\",\"--learning_rate\",\"0.01\",\"--model_dir\",\"s3://sagemaker-eu-west-1-663383416343/tensorflow-training-2021-04-29-18-02-23-924/model\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_INPUT=/opt/ml/input/data/input\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=128\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=s3://sagemaker-eu-west-1-663383416343/tensorflow-training-2021-04-29-18-02-23-924/model\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=50\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.01\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.7 train.py --batch_size 128 --epochs 50 --learning_rate 0.01 --model_dir s3://sagemaker-eu-west-1-663383416343/tensorflow-training-2021-04-29-18-02-23-924/model\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mData location: /opt/ml/input/data/input\u001b[0m\n",
      "\u001b[34mX train (303, 13) y train (303,)\u001b[0m\n",
      "\u001b[34mX test (102, 13) y test (102,)\u001b[0m\n",
      "\u001b[34m/cpu:0\u001b[0m\n",
      "\u001b[34mbatch_size = 128, epochs = 50, learning rate = 0.01\u001b[0m\n",
      "\u001b[34m[2021-04-29 18:05:14.531 ip-10-0-85-150.eu-west-1.compute.internal:24 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2021-04-29 18:05:14.753 ip-10-0-85-150.eu-west-1.compute.internal:24 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2021-04-29 18:05:14.847 ip-10-0-85-150.eu-west-1.compute.internal:24 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-04-29 18:05:14.847 ip-10-0-85-150.eu-west-1.compute.internal:24 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-04-29 18:05:14.848 ip-10-0-85-150.eu-west-1.compute.internal:24 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-04-29 18:05:14.848 ip-10-0-85-150.eu-west-1.compute.internal:24 INFO state_store.py:75] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-04-29 18:05:14.850 ip-10-0-85-150.eu-west-1.compute.internal:24 INFO hook.py:413] Monitoring the collections: losses, sm_metrics, metrics\u001b[0m\n",
      "\u001b[34mEpoch 1/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 550.1693 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 70ms/step - loss: 543.1182 - batch: 0.0000e+00 - val_loss: 441.6633 - val_batch: 0.0000e+00\u001b[0m\n",
      "\u001b[34mEpoch 2/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 429.6878#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 14ms/step - loss: 397.1483 - val_loss: 309.5216 - batch: 1.0000\u001b[0m\n",
      "\u001b[34mEpoch 3/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 293.8092#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 14ms/step - loss: 269.6598 - val_loss: 201.5405 - batch: 2.0000\u001b[0m\n",
      "\u001b[34mEpoch 4/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 175.9075#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 14ms/step - loss: 170.9539 - val_loss: 132.8710 - batch: 3.0000\u001b[0m\n",
      "\u001b[34mEpoch 5/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 114.0408#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 15ms/step - loss: 110.2618 - val_loss: 96.5645 - batch: 4.0000\u001b[0m\n",
      "\u001b[34mEpoch 6/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 80.3305#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 13ms/step - loss: 79.6949 - val_loss: 80.5426 - batch: 5.0000\u001b[0m\n",
      "\u001b[34mEpoch 7/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 61.2729#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 13ms/step - loss: 65.9225 - val_loss: 72.1087 - batch: 6.0000\u001b[0m\n",
      "\u001b[34mEpoch 8/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 45.9117#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 13ms/step - loss: 58.3308 - val_loss: 66.5820 - batch: 7.0000\u001b[0m\n",
      "\u001b[34mEpoch 9/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 53.3445#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 13ms/step - loss: 53.3533 - val_loss: 62.9359 - batch: 8.0000\u001b[0m\n",
      "\u001b[34mEpoch 10/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 43.7048#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 17ms/step - loss: 50.4067 - val_loss: 60.2891 - batch: 9.0000\u001b[0m\n",
      "\u001b[34mEpoch 11/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 58.5374#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 13ms/step - loss: 48.3461 - val_loss: 58.4468 - batch: 10.0000\u001b[0m\n",
      "\u001b[34mEpoch 12/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 49.5124#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 14ms/step - loss: 46.5676 - val_loss: 56.9030 - batch: 11.0000\u001b[0m\n",
      "\u001b[34mEpoch 13/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 40.1879#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 12ms/step - loss: 45.1235 - val_loss: 57.0423 - batch: 12.0000\u001b[0m\n",
      "\u001b[34mEpoch 14/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 42.7108#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 13ms/step - loss: 44.1243 - val_loss: 54.6158 - batch: 13.0000\u001b[0m\n",
      "\u001b[34mEpoch 15/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 43.6669#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 12ms/step - loss: 43.3392 - val_loss: 53.6271 - batch: 14.0000\u001b[0m\n",
      "\u001b[34mEpoch 16/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 41.9824#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 12ms/step - loss: 41.7377 - val_loss: 52.2468 - batch: 15.0000\u001b[0m\n",
      "\u001b[34mEpoch 17/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 41.8064#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 13ms/step - loss: 40.3873 - val_loss: 51.5868 - batch: 16.0000\u001b[0m\n",
      "\u001b[34mEpoch 18/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 26.7020#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 14ms/step - loss: 38.9651 - val_loss: 51.2245 - batch: 17.0000\u001b[0m\n",
      "\u001b[34mEpoch 19/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 39.0324#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 15ms/step - loss: 37.4875 - val_loss: 48.3100 - batch: 18.0000\u001b[0m\n",
      "\u001b[34mEpoch 20/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 27.3783#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 14ms/step - loss: 35.9469 - val_loss: 46.5889 - batch: 19.0000\u001b[0m\n",
      "\u001b[34mEpoch 21/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 38.7600#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 14ms/step - loss: 34.8125 - val_loss: 45.3835 - batch: 20.0000\u001b[0m\n",
      "\u001b[34mEpoch 22/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 48.8426#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 13ms/step - loss: 34.2878 - val_loss: 44.7334 - batch: 21.0000\u001b[0m\n",
      "\u001b[34mEpoch 23/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 41.0661#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 12ms/step - loss: 32.6568 - val_loss: 42.6814 - batch: 22.0000\u001b[0m\n",
      "\u001b[34mEpoch 24/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 40.5899#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 13ms/step - loss: 31.6074 - val_loss: 43.6867 - batch: 23.0000\u001b[0m\n",
      "\u001b[34mEpoch 25/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 25.5592#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 14ms/step - loss: 31.1049 - val_loss: 40.8869 - batch: 24.0000\u001b[0m\n",
      "\u001b[34mEpoch 26/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 31.2259#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 15ms/step - loss: 29.5954 - val_loss: 39.3382 - batch: 25.0000\u001b[0m\n",
      "\u001b[34mEpoch 27/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 23.9304#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 13ms/step - loss: 28.8328 - val_loss: 38.5531 - batch: 26.0000\u001b[0m\n",
      "\u001b[34mEpoch 28/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 21.1206#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 13ms/step - loss: 27.9596 - val_loss: 37.5250 - batch: 27.0000\u001b[0m\n",
      "\u001b[34mEpoch 29/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 27.5152#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 12ms/step - loss: 27.2627 - val_loss: 37.0119 - batch: 28.0000\u001b[0m\n",
      "\u001b[34mEpoch 30/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 32.6591#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 13ms/step - loss: 26.6236 - val_loss: 35.5976 - batch: 29.0000\u001b[0m\n",
      "\u001b[34mEpoch 31/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 33.4007#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 12ms/step - loss: 26.4795 - val_loss: 34.8243 - batch: 30.0000\u001b[0m\n",
      "\u001b[34mEpoch 32/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 18.3318#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 13ms/step - loss: 25.7148 - val_loss: 33.6936 - batch: 31.0000\u001b[0m\n",
      "\u001b[34mEpoch 33/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 19.5969#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 15ms/step - loss: 24.9147 - val_loss: 32.4130 - batch: 32.0000\u001b[0m\n",
      "\u001b[34mEpoch 34/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 21.7304#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 16ms/step - loss: 25.2315 - val_loss: 37.0443 - batch: 33.0000\u001b[0m\n",
      "\u001b[34mEpoch 35/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 23.9652#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 12ms/step - loss: 24.9281 - val_loss: 31.7144 - batch: 34.0000\u001b[0m\n",
      "\u001b[34mEpoch 36/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 33.0109#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 12ms/step - loss: 23.3834 - val_loss: 30.1480 - batch: 35.0000\u001b[0m\n",
      "\u001b[34mEpoch 37/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 25.3288#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 13ms/step - loss: 22.8829 - val_loss: 29.8962 - batch: 36.0000\u001b[0m\n",
      "\u001b[34mEpoch 38/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 23.3527#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 13ms/step - loss: 22.9083 - val_loss: 28.0795 - batch: 37.0000\u001b[0m\n",
      "\u001b[34mEpoch 39/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 21.9710#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 13ms/step - loss: 21.5277 - val_loss: 27.7762 - batch: 38.0000\u001b[0m\n",
      "\u001b[34mEpoch 40/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 23.0959#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 15ms/step - loss: 21.3742 - val_loss: 27.6163 - batch: 39.0000\u001b[0m\n",
      "\u001b[34mEpoch 41/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 14.9323#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 14ms/step - loss: 20.6012 - val_loss: 28.4986 - batch: 40.0000\u001b[0m\n",
      "\u001b[34mEpoch 42/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 28.0787#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 17ms/step - loss: 21.4712 - val_loss: 25.2369 - batch: 41.0000\u001b[0m\n",
      "\u001b[34mEpoch 43/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 18.9354#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 17ms/step - loss: 19.8648 - val_loss: 25.1023 - batch: 42.0000\u001b[0m\n",
      "\u001b[34mEpoch 44/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 17.8317#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 15ms/step - loss: 19.7589 - val_loss: 25.8922 - batch: 43.0000\u001b[0m\n",
      "\u001b[34mEpoch 45/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 18.5713#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 15ms/step - loss: 19.1052 - val_loss: 25.3537 - batch: 44.0000\u001b[0m\n",
      "\u001b[34mEpoch 46/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 17.2285#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 17ms/step - loss: 18.9844 - val_loss: 24.1860 - batch: 45.0000\u001b[0m\n",
      "\u001b[34mEpoch 47/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 19.8580#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 17ms/step - loss: 19.3449 - val_loss: 23.5741 - batch: 46.0000\u001b[0m\n",
      "\u001b[34mEpoch 48/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 19.2608#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 15ms/step - loss: 18.2457 - val_loss: 23.2414 - batch: 47.0000\u001b[0m\n",
      "\u001b[34mEpoch 49/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 19.5208#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 14ms/step - loss: 18.0411 - val_loss: 22.5689 - batch: 48.0000\u001b[0m\n",
      "\u001b[34mEpoch 50/50\u001b[0m\n",
      "\u001b[34m#0151/3 [=========>....................] - ETA: 0s - loss: 14.3305#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/3 [==============================] - 0s 13ms/step - loss: 17.2755 - val_loss: 22.3683 - batch: 49.0000\u001b[0m\n",
      "\u001b[34m1/1 - 0s - loss: 22.3683\n",
      "\u001b[0m\n",
      "\u001b[34mTest MSE : 22.368261337280273\u001b[0m\n",
      "\u001b[34m2021-04-29 18:05:13.067857: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2021-04-29 18:05:13.067986: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2021-04-29 18:05:13.094804: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\u001b[0m\n",
      "\u001b[34m2021-04-29 18:05:17.922966: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Assets written to: /opt/ml/model/1/assets\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Assets written to: /opt/ml/model/1/assets\n",
      "\u001b[0m\n",
      "\u001b[34m2021-04-29 18:05:18,588 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-04-29 18:05:47 Completed - Training job completed\n",
      "Training seconds: 49\n",
      "Billable seconds: 49\n"
     ]
    }
   ],
   "source": [
    "hosted_estimator.fit({'input':processed_dataset})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The training job produces a model saved in S3 that we can retrieve.  This is an example of the modularity of SageMaker: having trained the model in SageMaker, you can now take the model out of SageMaker and run it anywhere else.  Alternatively, you can deploy the model into a production-ready environment using SageMaker's hosted endpoints functionality, as shown in the **SageMaker hosted endpoint** section below.\n",
    "\n",
    "Retrieving the model from S3 is very easy:  the hosted training estimator you created above stores a reference to the model's location in S3.  You simply copy the model from S3 using the estimator's `model_data` property and unzip it to inspect the contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-eu-west-1-663383416343/tensorflow-training-2021-04-29-18-02-23-924/output/model.tar.gz to model/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {hosted_estimator.model_data} ./model/model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The unzipped archive should include the assets required by TensorFlow Serving to load the model and serve it, including a .pb file:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/\n",
      "1/variables/\n",
      "1/variables/variables.data-00000-of-00001\n",
      "1/variables/variables.index\n",
      "1/assets/\n",
      "1/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "!tar -xvzf ./model/model.tar.gz -C ./model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "    \n",
    "The final step in this pipeline is offline, batch scoring (inference/prediction).  The inputs to this step will be the model we trained earlier, and the test data.  A simple, ordinary Python script is all we need to do the actual batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......................................\u001b[34mINFO:__main__:starting services\u001b[0m\n",
      "\u001b[34mINFO:tfs_utils:using default model name: model\u001b[0m\n",
      "\u001b[34mINFO:tfs_utils:tensorflow serving model config: \u001b[0m\n",
      "\u001b[34mmodel_config_list: {\n",
      "  config: {\n",
      "    name: \"model\",\n",
      "    base_path: \"/opt/ml/model\",\n",
      "    model_platform: \"tensorflow\"\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:using default model name: model\u001b[0m\n",
      "\u001b[34mINFO:__main__:tensorflow serving model config: \u001b[0m\n",
      "\u001b[34mmodel_config_list: {\n",
      "  config: {\n",
      "    name: \"model\",\n",
      "    base_path: \"/opt/ml/model\",\n",
      "    model_platform: \"tensorflow\"\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:tensorflow version info:\u001b[0m\n",
      "\u001b[34mTensorFlow ModelServer: 2.3.0-rc0+dev.sha.no_git\u001b[0m\n",
      "\u001b[34mTensorFlow Library: 2.3.0\u001b[0m\n",
      "\u001b[34mINFO:__main__:tensorflow serving command: tensorflow_model_server --port=10000 --rest_api_port=10001 --model_config_file=/sagemaker/model-config.cfg --max_num_load_retries=0 \u001b[0m\n",
      "\u001b[34mINFO:__main__:started tensorflow serving (pid: 11)\u001b[0m\n",
      "\u001b[34mINFO:__main__:nginx config: \u001b[0m\n",
      "\u001b[34mload_module modules/ngx_http_js_module.so;\n",
      "\u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr info;\n",
      "\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\n",
      "\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/json;\n",
      "  access_log /dev/stdout combined;\n",
      "  js_include tensorflow-serving.js;\n",
      "\n",
      "  upstream tfs_upstream {\n",
      "    server localhost:10001;\n",
      "  }\n",
      "\n",
      "  upstream gunicorn_upstream {\n",
      "    server unix:/tmp/gunicorn.sock fail_timeout=1;\n",
      "  }\n",
      "\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    client_body_buffer_size 100m;\n",
      "    subrequest_output_buffer_size 100m;\n",
      "\n",
      "    set $tfs_version 2.3;\n",
      "    set $default_tfs_model model;\n",
      "\n",
      "    location /tfs {\n",
      "        rewrite ^/tfs/(.*) /$1  break;\n",
      "        proxy_redirect off;\n",
      "        proxy_pass_request_headers off;\n",
      "        proxy_set_header Content-Type 'application/json';\n",
      "        proxy_set_header Accept 'application/json';\n",
      "        proxy_pass http://tfs_upstream;\n",
      "    }\n",
      "\n",
      "    location /ping {\n",
      "        js_content ping;\n",
      "    }\n",
      "\n",
      "    location /invocations {\n",
      "        js_content invocations;\n",
      "    }\n",
      "\n",
      "    location /models {\n",
      "        proxy_pass http://gunicorn_upstream/models;\n",
      "    }\n",
      "\n",
      "    location / {\n",
      "        return 404 '{\"error\": \"Not Found\"}';\n",
      "    }\n",
      "\n",
      "    keepalive_timeout 3;\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:nginx version info:\u001b[0m\n",
      "\u001b[34mnginx version: nginx/1.18.0\u001b[0m\n",
      "\u001b[34mbuilt by gcc 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04) \u001b[0m\n",
      "\u001b[34mbuilt with OpenSSL 1.1.1  11 Sep 2018 (running with OpenSSL 1.1.1g  21 Apr 2020)\u001b[0m\n",
      "\u001b[34mTLS SNI support enabled\u001b[0m\n",
      "\u001b[34mconfigure arguments: --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt='-g -O2 -fdebug-prefix-map=/data/builder/debuild/nginx-1.18.0/debian/debuild-base/nginx-1.18.0=. -fstack-protector-strong -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fPIC' --with-ld-opt='-Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-z,now -Wl,--as-needed -pie'\u001b[0m\n",
      "\u001b[34mINFO:__main__:started nginx (pid: 13)\u001b[0m\n",
      "\u001b[34m2021/04/29 18:12:20 [notice] 13#13: using the \"epoll\" event method\u001b[0m\n",
      "\u001b[34m2021/04/29 18:12:20 [notice] 13#13: nginx/1.18.0\u001b[0m\n",
      "\u001b[34m2021/04/29 18:12:20 [notice] 13#13: built by gcc 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04) \u001b[0m\n",
      "\u001b[34m2021/04/29 18:12:20 [notice] 13#13: OS: Linux 4.14.225-169.362.amzn2.x86_64\u001b[0m\n",
      "\u001b[34m2021/04/29 18:12:20 [notice] 13#13: getrlimit(RLIMIT_NOFILE): 65536:99999\u001b[0m\n",
      "\u001b[34m2021/04/29 18:12:20 [notice] 13#13: start worker processes\u001b[0m\n",
      "\u001b[34m2021/04/29 18:12:20 [notice] 13#13: start worker process 14\u001b[0m\n",
      "\u001b[34m2021/04/29 18:12:20 [notice] 13#13: start worker process 15\u001b[0m\n",
      "\u001b[34m2021/04/29 18:12:20 [notice] 13#13: start worker process 16\u001b[0m\n",
      "\u001b[34m2021/04/29 18:12:20 [notice] 13#13: start worker process 17\u001b[0m\n",
      "\u001b[34m2021-04-29 18:12:20.911497: I tensorflow_serving/model_servers/server_core.cc:464] Adding/updating models.\u001b[0m\n",
      "\u001b[34m2021-04-29 18:12:20.911585: I tensorflow_serving/model_servers/server_core.cc:575]  (Re-)adding model: model\u001b[0m\n",
      "\u001b[34m2021-04-29 18:12:21.011761: I tensorflow_serving/util/retrier.cc:46] Retrying of Reserving resources for servable: {name: model version: 1} exhausted max_num_retries: 0\u001b[0m\n",
      "\u001b[34m2021-04-29 18:12:21.011791: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: model version: 1}\u001b[0m\n",
      "\u001b[34m2021-04-29 18:12:21.011797: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: model version: 1}\u001b[0m\n",
      "\u001b[34m2021-04-29 18:12:21.011804: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: model version: 1}\u001b[0m\n",
      "\u001b[34m2021-04-29 18:12:21.011827: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /opt/ml/model/1\u001b[0m\n",
      "\u001b[34m2021-04-29 18:12:21.013202: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\u001b[0m\n",
      "\u001b[34m2021-04-29 18:12:21.013229: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:234] Reading SavedModel debug info (if present) from: /opt/ml/model/1\u001b[0m\n",
      "\u001b[34m2021-04-29 18:12:21.013330: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX512F\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2021-04-29 18:12:21.014335: I external/org_tensorflow/tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\u001b[0m\n",
      "\u001b[34m2021-04-29 18:12:21.040748: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:199] Restoring SavedModel bundle.\u001b[0m\n",
      "\u001b[34m2021-04-29 18:12:21.078781: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:183] Running initialization op on SavedModel bundle at path: /opt/ml/model/1\u001b[0m\n",
      "\u001b[34m2021-04-29 18:12:21.083062: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:303] SavedModel load for tags { serve }; Status: success: OK. Took 71228 microseconds.\u001b[0m\n",
      "\u001b[34m2021-04-29 18:12:21.083532: I tensorflow_serving/servables/tensorflow/saved_model_warmup_util.cc:59] No warmup data file found at /opt/ml/model/1/assets.extra/tf_serving_warmup_requests\u001b[0m\n",
      "\u001b[34m2021-04-29 18:12:21.083840: I tensorflow_serving/util/retrier.cc:46] Retrying of Loading servable: {name: model version: 1} exhausted max_num_retries: 0\u001b[0m\n",
      "\u001b[34m2021-04-29 18:12:21.083856: I tensorflow_serving/core/loader_harness.cc:87] Successfully loaded servable version {name: model version: 1}\u001b[0m\n",
      "\u001b[34m2021-04-29 18:12:21.085709: I tensorflow_serving/model_servers/server.cc:367] Running gRPC ModelServer at 0.0.0.0:10000 ...\u001b[0m\n",
      "\u001b[34m[warn] getaddrinfo: address family for nodename not supported\u001b[0m\n",
      "\u001b[34m2021-04-29 18:12:21.086765: I tensorflow_serving/model_servers/server.cc:387] Exporting HTTP/REST API at:localhost:10001 ...\u001b[0m\n",
      "\u001b[34m[evhttp_server.cc : 238] NET_LOG: Entering the event loop ...\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [29/Apr/2021:18:12:24 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [29/Apr/2021:18:12:24 +0000] \"GET /execution-parameters HTTP/1.1\" 404 22 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m2021/04/29 18:12:24 [info] 14#14: *1 client 169.254.255.130 closed keepalive connection\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [29/Apr/2021:18:12:24 +0000] \"POST /invocations HTTP/1.1\" 200 1412 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\n",
      "\u001b[32m2021-04-29T18:12:24.733:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "batch_input = 's3://{}/boston-housing/data/processed/X_val.json'.format(bucket)\n",
    "batch_output = 's3://{}/boston-housing/batch-output'.format(bucket)\n",
    "\n",
    "tf_transformer = hosted_estimator.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    output_path=batch_output\n",
    ")\n",
    "\n",
    "tf_transformer.transform(data=batch_input, content_type='application/json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-eu-west-1-663383416343/boston-housing/batch-output/X_val.json.out to ./X_val.json.out\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive $tf_transformer.output_path ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-eu-west-1-663383416343/boston-housing/data/processed/y_val.json to ./y_val.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://$bucket/boston-housing/data/processed/y_val.json ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open(\"X_val.json.out\", \"r\") as read_file:\n",
    "    validation_pred = json.load(read_file)\n",
    "    validation_pred = [item for item_arr in validation_pred['predictions'] for item in item_arr]\n",
    "\n",
    "with open(\"y_val.json\", \"r\") as read_file:\n",
    "    validation_true = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE: 17.469291181972036\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(validation_true, validation_pred)\n",
    "print('Validation MSE: {}'.format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Automatic Model Tuning\n",
    "\n",
    "So far we have simply run one Hosted Training job without any real attempt to tune hyperparameters to produce a better model.  Selecting the right hyperparameter values to train your model can be difficult, and typically is very time consuming if done manually. The right combination of hyperparameters is dependent on your data and algorithm; some algorithms have many different hyperparameters that can be tweaked; some are very sensitive to the hyperparameter values selected; and most have a non-linear relationship between model fit and hyperparameter values.  SageMaker Automatic Model Tuning helps automate the hyperparameter tuning process:  it runs multiple training jobs with different hyperparameter combinations to find the set with the best model performance.\n",
    "\n",
    "We begin by specifying the hyperparameters we wish to tune, and the range of values over which to tune each one.  We also must specify an objective metric to be optimized:  in this use case, we'd like to minimize the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "  'learning_rate': ContinuousParameter(0.001, 0.2, scaling_type=\"Logarithmic\"),\n",
    "  'epochs': IntegerParameter(20, 90),\n",
    "  'batch_size': IntegerParameter(64, 256),\n",
    "}\n",
    "\n",
    "metric_definitions = [{'Name': 'loss',\n",
    "                       'Regex': ' loss: ([0-9\\\\.]+)'},\n",
    "                     {'Name': 'val_loss',\n",
    "                       'Regex': ' val_loss: ([0-9\\\\.]+)'}]\n",
    "\n",
    "objective_metric_name = 'val_loss'\n",
    "objective_type = 'Minimize'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Next we specify a HyperparameterTuner object that takes the above definitions as parameters.  Each tuning job must be given a budget:  a maximum number of training jobs.  A tuning job will complete after that many training jobs have been executed.  \n",
    "\n",
    "We also can specify how much parallelism to employ, in this case five jobs, meaning that the tuning job will complete after three series of five jobs in parallel have completed.  For the default Bayesian Optimization tuning strategy used here, the tuning search is informed by the results of previous groups of training jobs, so we don't run all of the jobs in parallel, but rather divide the jobs into groups of parallel jobs.  There is a trade-off: using more parallel jobs will finish tuning sooner, but likely will sacrifice tuning search accuracy. \n",
    "\n",
    "Now we can launch a hyperparameter tuning job by calling the `fit` method of the HyperparameterTuner object.  The tuning job may take around 10 minutes to finish.  While you're waiting, the status of the tuning job, including metadata and results for invidual training jobs within the tuning job, can be checked in the SageMaker console in the **Hyperparameter tuning jobs** panel.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................................................................................................................!\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "tuner = HyperparameterTuner(hosted_estimator,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            metric_definitions,\n",
    "                            max_jobs=10,\n",
    "                            max_parallel_jobs=4,\n",
    "                            objective_type=objective_type)\n",
    "\n",
    "tuner.fit({'input':processed_dataset}, job_name='boston-housing-tuning')\n",
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "After the tuning job is finished, we can use the `HyperparameterTuningJobAnalytics` object from the SageMaker Python SDK to list the top 5 tuning jobs with the best performance. Although the results vary from tuning job to tuning job, the best validation loss from the tuning job (under the FinalObjectiveValue column) likely will be substantially lower than the validation loss from the hosted training job above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>TrainingJobName</th>\n",
       "      <th>TrainingJobStatus</th>\n",
       "      <th>FinalObjectiveValue</th>\n",
       "      <th>TrainingStartTime</th>\n",
       "      <th>TrainingEndTime</th>\n",
       "      <th>TrainingElapsedTimeSeconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>83.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.140197</td>\n",
       "      <td>boston-housing-tuning-005-55d6172c</td>\n",
       "      <td>Completed</td>\n",
       "      <td>11.9774</td>\n",
       "      <td>2021-04-29 18:43:33+00:00</td>\n",
       "      <td>2021-04-29 18:44:48+00:00</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.054352</td>\n",
       "      <td>boston-housing-tuning-009-1cb348b4</td>\n",
       "      <td>Completed</td>\n",
       "      <td>14.2057</td>\n",
       "      <td>2021-04-29 18:47:14+00:00</td>\n",
       "      <td>2021-04-29 18:48:10+00:00</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>135.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.042981</td>\n",
       "      <td>boston-housing-tuning-010-95b19d8b</td>\n",
       "      <td>Completed</td>\n",
       "      <td>14.8028</td>\n",
       "      <td>2021-04-29 18:47:20+00:00</td>\n",
       "      <td>2021-04-29 18:48:11+00:00</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>243.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.041792</td>\n",
       "      <td>boston-housing-tuning-003-7a856233</td>\n",
       "      <td>Completed</td>\n",
       "      <td>21.4786</td>\n",
       "      <td>2021-04-29 18:40:32+00:00</td>\n",
       "      <td>2021-04-29 18:41:19+00:00</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>214.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.076307</td>\n",
       "      <td>boston-housing-tuning-008-9f0eea63</td>\n",
       "      <td>Completed</td>\n",
       "      <td>22.7185</td>\n",
       "      <td>2021-04-29 18:43:40+00:00</td>\n",
       "      <td>2021-04-29 18:45:06+00:00</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch_size  epochs  learning_rate                     TrainingJobName  \\\n",
       "5        83.0    59.0       0.140197  boston-housing-tuning-005-55d6172c   \n",
       "1       144.0    79.0       0.054352  boston-housing-tuning-009-1cb348b4   \n",
       "0       135.0    50.0       0.042981  boston-housing-tuning-010-95b19d8b   \n",
       "7       243.0    28.0       0.041792  boston-housing-tuning-003-7a856233   \n",
       "2       214.0    22.0       0.076307  boston-housing-tuning-008-9f0eea63   \n",
       "\n",
       "  TrainingJobStatus  FinalObjectiveValue         TrainingStartTime  \\\n",
       "5         Completed              11.9774 2021-04-29 18:43:33+00:00   \n",
       "1         Completed              14.2057 2021-04-29 18:47:14+00:00   \n",
       "0         Completed              14.8028 2021-04-29 18:47:20+00:00   \n",
       "7         Completed              21.4786 2021-04-29 18:40:32+00:00   \n",
       "2         Completed              22.7185 2021-04-29 18:43:40+00:00   \n",
       "\n",
       "            TrainingEndTime  TrainingElapsedTimeSeconds  \n",
       "5 2021-04-29 18:44:48+00:00                        75.0  \n",
       "1 2021-04-29 18:48:10+00:00                        56.0  \n",
       "0 2021-04-29 18:48:11+00:00                        51.0  \n",
       "7 2021-04-29 18:41:19+00:00                        47.0  \n",
       "2 2021-04-29 18:45:06+00:00                        86.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner_metrics = sagemaker.HyperparameterTuningJobAnalytics('boston-housing-tuning')\n",
    "tuner_metrics.dataframe().sort_values(['FinalObjectiveValue'], ascending=True).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The total training time and training jobs status can be checked with the following lines of code. Because automatic early stopping is by default off, all the training jobs should be completed normally.  For an example of a more in-depth analysis of a tuning job, see the SageMaker official sample [HPO_Analyze_TuningJob_Results.ipynb](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/hyperparameter_tuning/analyze_results/HPO_Analyze_TuningJob_Results.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total training time is 0.16 hours\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Completed    10\n",
       "Name: TrainingJobStatus, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_time = tuner_metrics.dataframe()['TrainingElapsedTimeSeconds'].sum() / 3600\n",
    "print(\"The total training time is {:.2f} hours\".format(total_time))\n",
    "tuner_metrics.dataframe()['TrainingJobStatus'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Endpoint deployment\n",
    "\n",
    "Assuming the best model from the tuning job is better than the model produced by the individual hosted training job above, we could now easily deploy that model to production.  A convenient option is to use a SageMaker hosted endpoint, which serves real time predictions from the trained model (For asynchronous, offline predictions on large datasets, you can use either SageMaker Processing or SageMaker Batch Transform.). The endpoint will retrieve the TensorFlow SavedModel created during training and deploy it within a SageMaker TensorFlow Serving container. This all can be accomplished with one line of code.  \n",
    "\n",
    "More specifically, by calling the `deploy` method of the HyperparameterTuner object we instantiated above, we can directly deploy the best model from the tuning job to a SageMaker hosted endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-eu-west-1-663383416343/boston-housing/data/processed/StandardScaler.pkl to code/StandardScaler.pkl\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://$bucket/boston-housing/data/processed/StandardScaler.pkl ./code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-04-29 18:44:48 Starting - Preparing the instances for training\n",
      "2021-04-29 18:44:48 Downloading - Downloading input data\n",
      "2021-04-29 18:44:48 Training - Training image download completed. Training in progress.\n",
      "2021-04-29 18:44:48 Uploading - Uploading generated training model\n",
      "2021-04-29 18:44:48 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "best_model = tuner.best_estimator().create_model(entry_point='inference.py',\n",
    "              source_dir='code/inference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "update_endpoint is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Using already existing model: boston-housing-tuning-005-55d6172c-2021-04-29-18-48-52-792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------!"
     ]
    }
   ],
   "source": [
    "tuning_predictor = best_model.deploy(initial_instance_count=1, instance_type='ml.m5.large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "We can compare the predictions generated by this endpoint with the actual target values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: \t[31.8 24.  32.3 31.7 31.  27.  22.  22.  17.5 20.8]\n",
      "target values: \t[24.  21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9]\n"
     ]
    }
   ],
   "source": [
    "results = tuning_predictor.predict({'instances': X[:10]})['predictions']\n",
    "\n",
    "flat_list = [float('%.1f'%(item)) for sublist in results for item in sublist]\n",
    "\n",
    "print('predictions: \\t{}'.format(np.array(flat_list)))\n",
    "print('target values: \\t{}'.format(y[:10].round(decimals=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How would the code look like in your application? \n",
    "\n",
    "Here is a sample function that accepts property parameters as function input and returns predicted property value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_house_value(crim, zn, indus, chas, nox, rm, age, dis, rad, tax, ptratio, b, lstat):\n",
    "    \"\"\"Predict housing value in $1000's\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    crim : float\n",
    "        per capita crime rate by town\n",
    "    zn : float\n",
    "        proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "    indus : float\n",
    "        proportion of non-retail business acres per town\n",
    "    chas : int\n",
    "        Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "    nox : float\n",
    "        nitric oxides concentration (parts per 10 million)\n",
    "    rm : float\n",
    "        average number of rooms per dwelling\n",
    "    age : float\n",
    "        proportion of owner-occupied units built prior to 1940\n",
    "    dis : float\n",
    "        weighted distances to five Boston employment centres\n",
    "    rad : float\n",
    "        index of accessibility to radial highways\n",
    "    tax : float\n",
    "        full-value property-tax rate per $10,000\n",
    "    ptratio : float\n",
    "        pupil-teacher ratio by town\n",
    "    b : float\n",
    "        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "    lstat : float\n",
    "        % lower status of the population\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Median value of owner-occupied homes in $1000s\n",
    "    \"\"\"\n",
    "    model_input = {\n",
    "        'instances': [\n",
    "            [crim, zn, indus, chas, nox, rm, age, dis, rad, tax, ptratio, b, lstat]\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    return tuning_predictor.predict(model_input)['predictions'][0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted house value: $31.77k\n"
     ]
    }
   ],
   "source": [
    "predicted_value = predict_house_value(0.01, 18.0, 2.31, 0, 0.54, 6.58, 65.2, 4.09, 1.0, 296.0, 15.3, 396.9, 4.98)\n",
    "print('Predicted house value: ${:.2f}k'.format(predicted_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/tensorflow-2.3-cpu-py37-ubuntu18.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
